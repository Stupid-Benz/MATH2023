\documentclass[11pt, a4paper]{book}

%--------------------Basic Package--------------------%
\usepackage{amsthm, amssymb, amsmath, graphicx}
\graphicspath{{figures/}}

%----------------------Hyperref-----------------------%
\usepackage[usenames,dvipsnames]{xcolor}
\definecolor{ocre}{RGB}{0,83,166}
\definecolor{cite}{HTML}{11871E}
\definecolor{url}{HTML}{698996}
\definecolor{link}{HTML}{912F1B}

\usepackage[pdfencoding=unicode, colorlinks=true, linkcolor=link, citecolor=cite, urlcolor=url, linktocpage]{hyperref}

%------------------------Tikz-------------------------%
\usepackage{tikz}
\usetikzlibrary{cd}

%-----------------------------------------------------%
%%%% Page setup
\usepackage[
  a4paper,
  twoside=true,
  textheight=22cm,
  textwidth=15cm,
  marginparsep=0.75cm,
  marginparwidth=2.5cm,
  heightrounded,
  centering
]{geometry}

%-----------------------------------------------------%
%%%% Fonts
\usepackage[protrusion=true]{microtype}

\usepackage[bitstream-charter]{mathdesign}
\usepackage[T1]{fontenc}
\usepackage{biolinum}
\renewcommand{\mathsf}[1]{\text{\normalfont\sffamily#1}}
\newcommand{\mathsfbf}[1]{\text{\normalfont\bf\sffamily#1}}

\DeclareMathAlphabet{\eur}{U}{zeus}{m}{n}
\renewcommand{\mathcal}[1]{\eur{#1}}

%-----------------------------------------------------%
\usepackage{subcaption} % for subcaption

\usepackage{booktabs}

%-------------------Theorem Styles--------------------%
\theoremstyle{plain}
\newtheorem{example}{Example}[chapter]
\theoremstyle{definition}
\newtheorem{definition}{Definition}[chapter]
\newtheorem{theorem}{Theorem}[chapter]
% Non-italic
\theoremstyle{remark}
\newenvironment{solution}{\begin{proof}[Solution]}{\end{proof}}
\newtheorem*{remark}{Remark}

%Added some common sets
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}

\usetikzlibrary{cd}

\begin{document}

\frontmatter
\title{\sffamily\Huge\color{ocre} Multivariable Calculus}
\author{\sffamily Department of Mathematics\\\sffamily Hong Kong University of Science and Technology}
\date{\sffamily \today}
\maketitle

\tableofcontents

\mainmatter{}

\chapter{Vectors and the Geometry of Space}

\section{Three-Dimensional Coordinate Systems}

We would use an ordered tuple of three numbers $(x, y, z)$ to represent a point in three-dimensional space. The three numbers correspond to the distances along the $x$-axis, $y$-axis, and $z$-axis respectively.

Moreover, we can use a vector to represent a point in space. A vector $\mathbf{v}$ can be expressed as:
\[
  \mathbf{v} = \langle x, y, z \rangle = x\mathbf{i} + y\mathbf{j} + z\mathbf{k}
\]
where $\mathbf{i}$, $\mathbf{j}$, and $\mathbf{k}$ are the unit vectors along the $x$-, $y$-, and $z$-axes respectively.

\begin{remark}
  Unit vectors are vectors with a magnitude of 1. They are often used to indicate direction.
\end{remark}

The distance, or norm, of the vector $\mathbf{v}$ from the origin can be calculated using the formula:
\[
  \|\mathbf{v}\|_2 = \|\mathbf{v}\| = \sqrt{x^2 + y^2 + z^2}
\]
This is also known as the Euclidean norm.

As we are used to consider two-dimensional planes, we always consider the following equations as circles in two-dimensional space:
\[
  x^2 + y^2 = r^2
\]
However, in three-dimensional space, this equation represents a cylinder extending infinitely along the $z$-axis. As implicitly, the equation does not restrict the value of $z$. Then the set of points satisfying the equation forms a cylinder.

In two-dimensional case, the set of points satisfying the equation $x^2 + y^2 = r^2$ represents a circle of radius $r$ centered at the origin:
\[
  S^1 = \{ (x, y) \mid x^2 + y^2 = r^2 \}
\]
In three-dimensional case, the set of points satisfying the equation $x^2 + y^2 = r^2$ represents a cylinder of radius $r$ centered along the $z$-axis:
\[
  C = \{ (x, y, z) \mid x^2 + y^2 = r^2, z \in \mathbb{R} \}
\]

So if we want to represent a two-dimensional circle in three-dimensional space, we need to add an additional constraint on $z$. For example, the set of points satisfying the equations $x^2 + y^2 = r^2$ and $z = 0$ represents a circle of radius $r$ in the $xy$-plane:
\[
  S^1 = \{ (x, y, z) \mid x^2 + y^2 = r^2, z = 0 \}
\]

For vector operations, we have:
\begin{itemize}
  \item Vector Addition: $\mathbf{a} + \mathbf{b} = \langle a_1 + b_1, a_2 + b_2, a_3 + b_3 \rangle$
  \item Scalar Multiplication: $c\mathbf{a} = \langle ca_1, ca_2, ca_3 \rangle$
\end{itemize}

Also, we have the dot product and cross product defined as:
\begin{itemize}
  \item Dot Product: $\mathbf{a} \cdot \mathbf{b} = a_1b_1 + a_2b_2 + a_3b_3$
  \item Cross Product: $\mathbf{a} \times \mathbf{b} = \langle a_2b_3 - a_3b_2, a_3b_1 - a_1b_3, a_1b_2 - a_2b_1 \rangle$
\end{itemize}

Moreover, the dot product can also be expressed in terms of the magnitudes of the vectors and the angle $\theta$ between them:
\[
  \mathbf{a} \cdot \mathbf{b} = \|\mathbf{a}\| \|\mathbf{b}\| \cos\theta
\]
and the magnitude of the cross product can be expressed as:
\[
  \|\mathbf{a} \times \mathbf{b}\| = \|\mathbf{a}\| \|\mathbf{b}\| \sin\theta
\]
It represents the area of the parallelogram formed by the two vectors.

If we want to project vector $\mathbf{b}$ onto vector $\mathbf{a}$, we can use the formula:
\[
  \text{proj}_{\mathbf{a}} \mathbf{b} = \left(\frac{\mathbf{a} \cdot \mathbf{b}}{\|\mathbf{a}\|}\right) \frac{\mathbf{a}}{\|\mathbf{a}\|} = \frac{\mathbf{a} \cdot \mathbf{b}}{\|\mathbf{a}\|^2} \mathbf{a}
\]
The scalar projection of $\mathbf{b}$ onto $\mathbf{a}$ is given by:
\[
  \text{comp}_{\mathbf{a}} \mathbf{b} = \|\mathbf{b}\| \cos\theta = \frac{\mathbf{a} \cdot \mathbf{b}}{\|\mathbf{a}\|}
\]

For the cross product, we can use the following determinant form:
\[
  \mathbf{a} \times \mathbf{b} = \begin{vmatrix}
    \mathbf{i} & \mathbf{j} & \mathbf{k} \\
    a_1        & a_2        & a_3        \\
    b_1        & b_2        & b_3
  \end{vmatrix}
\]

\begin{remark}
  The cross product of two vectors results in a vector that is orthogonal (perpendicular) to both original vectors. The direction of the resulting vector is determined by the right-hand rule.
\end{remark}

\section{Lines and Planes}

\subsection{Lines}

To represent a line in three-dimensional space, we can use a point and a direction vector. If we have a point $P_0(x_0, y_0, z_0)$ on the line and a direction vector $\mathbf{v} = \langle v_1, v_2, v_3 \rangle$, then any point $P(x, y, z)$ on the line, the vector $\overrightarrow{P_0P}$ is parallel to $\mathbf{v}$, i.e., $\overrightarrow{P_0P} = t\mathbf{v}$ for some scalar $t$. Then we have the parametric equations of the line as:
\[
  \langle x, y, z \rangle - \langle x_0, y_0, z_0 \rangle = t \langle v_1, v_2, v_3 \rangle
\]
or equivalently,
\[
  \begin{cases}
    x = x_0 + tv_1 \\
    y = y_0 + tv_2 \\
    z = z_0 + tv_3
  \end{cases}
\]
which are called the \emph{parametric equations} of the line. The $t$ is called the \emph{parameter} of the line.

To visualise the parametric equation of a line in 3D, consider Figure~\ref{fig:3d_line_parametric} below.
\begin{figure}[!ht]
  \centering
  \includegraphics{2d_plane_equation.pdf}
  \caption{Parametric Equation of a Line in 3D}\label{fig:3d_line_parametric}
\end{figure}

From Figure~\ref{fig:3d_line_parametric}, we can also write the parametric equations as:
\[
  \mathbf{r}(t) = \overrightarrow{OP_0} + t\mathbf{v} = \langle x_0, y_0, z_0 \rangle + t \langle v_1, v_2, v_3 \rangle
\]
which is called the \emph{vector form} of the line.

If $\mathbf{v} = \langle v_1, v_2, v_3 \rangle$ where none of $v_1, v_2, v_3$ is zero, we can also express the line in \emph{symmetric form} as:
\[
  \frac{x - x_0}{v_1} = \frac{y - y_0}{v_2} = \frac{z - z_0}{v_3}
\]

\begin{example}
  Find the parametric equations of the line that passes through the points $A(1, 2, 3)$ and $B(4, 5, 6)$. Express the line in vector form, parametric form and symmetric forms.
\end{example}
\begin{solution}
  In order to find the equation of the line, we need
  \begin{itemize}
    \item A point on the line: $A(1, 2, 3)$;
    \item A direction vector: $\mathbf{v} = \overrightarrow{AB} = \langle 4 - 1, 5 - 2, 6 - 3 \rangle = \langle 3, 3, 3 \rangle$.
  \end{itemize}
  Therefore, the vector form of the line is:
  \[
    \mathbf{r}(t) = \langle 1, 2, 3 \rangle + t \langle 3, 3, 3 \rangle
  \]
  The parametric form of the line is:
  \[
    x = 1 + 3t, \quad y = 2 + 3t, \quad z = 3 + 3t.
  \]
  The symmetric form of the line is:
  \[
    \frac{x - 1}{3} = \frac{y - 2}{3} = \frac{z - 3}{3}
  \]
\end{solution}

\begin{example}
  Find the parametric equations for the line passes through the point $P(0, 1, 2)$ that is perpendicular to and intersects the line
  \[
    x = 1 + t, \quad y = 1 - t, \quad z = 2t.
  \]
\end{example}
\begin{solution}
  We can assume the point of intersection is $Q(1 + t_0, 1 - t_0, 2t_0)$. The vector $\overrightarrow{PQ}$ is perpendicular to the direction vector of the given line $\mathbf{v} = \langle 1, -1, 2 \rangle$. Therefore, we have:
  \[
    \begin{split}
      \overrightarrow{PQ} \cdot \mathbf{v}                                                  & = 0            \\
      \langle (1 + t_0) - 0, (1 - t_0) - 1, 2t_0 - 2 \rangle \cdot \langle 1, -1, 2 \rangle & = 0            \\
      t_0                                                                                   & = \frac{1}{2}.
    \end{split}
  \]
  So the direction vector of the line we want is:
  \[
    \overrightarrow{PQ} = \left\langle 1 + \frac{1}{2}, 1 - \frac{1}{2} - 1, 2 \cdot \frac{1}{2} - 2 \right\rangle = \left\langle \frac{3}{2}, -\frac{1}{2}, -1 \right\rangle.
  \]
  Then we take the direction vector as $\langle 3, -1, -2 \rangle$. Therefore, the parametric equations of the line is:
  \[
    x = 3t, \quad y = 1 - t, \quad z = 2 - 2t.
  \]
\end{solution}

There are 4 types of lines in 3D space:
\begin{itemize}
  \item Intersecting Lines: Two lines that intersect at a single point.
  \item Parallel Lines: Two lines that never intersect and are always the same distance apart.
  \item Skew Lines: Two lines that do not intersect and are not parallel. They exist in different planes.
  \item Coincident Lines: Two lines that lie on top of each other, meaning they have all points in common.
\end{itemize}

\begin{example}
  Find the distance from the point $P_0$ to the straight line $L$ that passes through the point $P_1$ with the non-zero direction vector $\mathbf{v}$.
\end{example}
\begin{solution}
  Let $\mathbf{r_0}$ and $\mathbf{r_1}$ be the position vectors of the points $P_0$ and $P_1$ respectively. Let the point $P_2$ on the line $L$ such that $\overrightarrow{P_0P_2}$ is perpendicular to the direction vector $\mathbf{v}$. Then the distance from the point $P_0$ to the line $L$ is given by the length of the vector $\overrightarrow{P_0P_2}$. We have:
  \[
    \text{Distance} = \| \overrightarrow{P_0P_2} \| = \| \overrightarrow{P_0P_1} \| \sin{\theta}
  \]
  where $\theta$ is the angle between the vectors $\overrightarrow{P_0P_1}$ and $\mathbf{v}$. Using the definition of the cross product, we have:
  \[
    \| \overrightarrow{P_0P_1} \times \mathbf{v} \| = \| \overrightarrow{P_0P_1} \| \| \mathbf{v} \| \sin{\theta}.
  \]
  Hence, the distance from the point $P_0$ to the line $L$ is given by:
  \[
    \text{Distance} = \frac{\| \overrightarrow{P_0P_1} \times \mathbf{v} \|}{\| \mathbf{v} \|} = \frac{\| (\mathbf{r_1} - \mathbf{r_0}) \times \mathbf{v} \|}{\| \mathbf{v} \|}.
  \]
\end{solution}

\begin{example}
  Find the distance between the two lines $L_1$ through point $P_1$ parallel to direction vector $\mathbf{v_1}$ and $L_2$ through point $P_2$ parallel to direction vector $\mathbf{v_2}$.
\end{example}
\begin{figure}[!ht]
  \centering
  \includegraphics{skew_lines.pdf}
  \caption{Skew Lines in 3D Space}\label{fig:skew_lines}
\end{figure}
\begin{solution}
  Consider Figure~\ref{fig:skew_lines}. Let $\mathbf{r_1}$ and $\mathbf{r_2}$ be the position vectors of the points $P_1$ and $P_2$ respectively. Let $\mathbf{n} = \mathbf{v_1} \times \mathbf{v_2}$ be a vector orthogonal to both direction vectors $\mathbf{v_1}$ and $\mathbf{v_2}$. Then we take the vector $\overrightarrow{P_1P_2} = \mathbf{r_2} - \mathbf{r_1}$. The distance between the two lines $L_1$ and $L_2$ is given by the length of the projection of the vector $\overrightarrow{P_1P_2}$ onto the vector $\mathbf{n}$. We have:
  \[
    \text{Distance} = \| \text{proj}_{\mathbf{n}} \overrightarrow{P_1P_2} \| = \frac{|\overrightarrow{P_1P_2} \cdot \mathbf{n}|}{\|\mathbf{n}\|} = \frac{|(\mathbf{r_2} - \mathbf{r_1}) \cdot (\mathbf{v_1} \times \mathbf{v_2})|}{\|\mathbf{v_1} \times \mathbf{v_2}\|}.
  \]
\end{solution}

\subsection{Planes}

A plane in three-dimensional space can be defined using a point and a normal vector. If we have a point $P_0(x_0, y_0, z_0)$ on the plane and a normal vector $\mathbf{n} = \langle A, B, C \rangle$, then any point $P(x, y, z)$ on the plane satisfies the condition that the vector $\overrightarrow{P_0P}$ is orthogonal to the normal vector $\mathbf{n}$, i.e., $\mathbf{n} \cdot \overrightarrow{P_0P} = 0$. This leads to the equation of the plane:
\[
  \langle A, B, C \rangle \cdot (\langle x, y, z \rangle - \langle x_0, y_0, z_0 \rangle) = 0
\]
or equivalently,
\[
  A(x - x_0) + B(y - y_0) + C(z - z_0) = 0
\]
which is called the \emph{scalar equation} of the plane.

Expanding this, we get:
\[
  Ax + By + Cz = Ax_0 + By_0 + Cz_0
\]
or equivalently,
\[
  Ax + By + Cz + D = 0
\]
where $D = -(Ax_0 + By_0 + Cz_0)$ is a constant. It is called a \emph{linear equation} in $x$, $y$ and $z$.

To visualise the equation of a plane in 3D, consider Figure~\ref{fig:3d_plane_equation} below.
\begin{figure}[!ht]
  \centering
  \includegraphics{3d_plane_equation.pdf}
  \caption{Equation of a Plane in 3D}\label{fig:3d_plane_equation}
\end{figure}

In order to find $\mathbf{n}$, we can use the cross product.
\begin{example}
  Find the equation of the plane that passes through the points:
  \[
    A(1, 2, 3), \quad B(4, 5, 6), \quad C(7, 8, 0).
  \]
\end{example}
\begin{solution}
  In order to find the equation of the plane, we need
  \begin{itemize}
    \item A point on the plane: $A(1, 2, 3)$;
    \item A normal vector: $\mathbf{n} = \overrightarrow{AB} \times \overrightarrow{AC}$.
  \end{itemize}
  First, we calculate the vectors $\overrightarrow{AB}$ and $\overrightarrow{AC}$:
  \[
    \begin{split}
      \overrightarrow{AB} & = \langle 4 - 1, 5 - 2, 6 - 3 \rangle = \langle 3, 3, 3 \rangle,  \\
      \overrightarrow{AC} & = \langle 7 - 1, 8 - 2, 0 - 3 \rangle = \langle 6, 6, -3 \rangle.
    \end{split}
  \]
  Taking the cross product, we have:
  \[
    \overrightarrow{AB} \times \overrightarrow{AC} = \begin{vmatrix}
      \mathbf{i} & \mathbf{j} & \mathbf{k} \\
      3          & 3          & 3          \\
      6          & 6          & -3
    \end{vmatrix} = \langle 0, 0, -9 \rangle.
  \]
  For simplicity, we can take the normal vector as $\mathbf{n} = \langle 0, 0, 1 \rangle$. Therefore, the equation of the plane is:
  \[
    \begin{split}
      0(x - 1) + 0(y - 2) + 1(z - 3) & = 0  \\
      z - 3                          & = 0  \\
      z                              & = 3.
    \end{split}
  \]
\end{solution}

If we have a point $P_1(x_1, y_1, z_1)$ not on the plane, we can calculate the distance from the point to the plane using the formula:
\[
  \text{Distance} = \frac{\| \mathbf{n} \cdot \mathbf{b} \|}{\| \mathbf{n} \|} = \frac{A(x_1 - x_0) + B(y_1 - y_0) + C(z_1 - z_0)}{\sqrt{A^2 + B^2 + C^2}} = \frac{|A x_1 + B y_1 + C z_1 + D|}{\sqrt{A^2 + B^2 + C^2}}
\]
where $\mathbf{b} = \overrightarrow{P_0P_1} = \langle x_1 - x_0, y_1 - y_0, z_1 - z_0 \rangle$.

\begin{example}
  Let $L_1$ be the line through the points $(1, 2, 6)$ and $(2, 4, 8)$. Let $L_2$ be the of intersection of the planes $\pi_1$ and $\pi_2$, where $\pi_1$ is the plane $x - y + 2z + 1 = 0$ and $\pi_2$ is the plane through the points $(3, 2, -1)$, $(0, 0, 1)$ and $(1, 2, 1)$. Calculate the distance between the lines $L_1$ and $L_2$.
\end{example}
\begin{solution}
  First, we find the direction vector of the line $L_1$:
  \[
    \mathbf{v_1} = \langle 2 - 1, 4 - 2, 8 - 6 \rangle = \langle 1, 2, 2 \rangle.
  \]

  We know that the normal vector of the plane $\pi_1$ is $\mathbf{n_1} = \langle 1, -1, 2 \rangle$. Then find two vectors on the plane $\pi_2$:
  \[
    \begin{split}
      \overrightarrow{P_1P_2} & = \langle 0 - 3, 0 - 2, 1 - (-1) \rangle = \langle -3, -2, 2 \rangle, \\
      \overrightarrow{P_1P_3} & = \langle 1 - 3, 2 - 2, 1 - (-1) \rangle = \langle -2, 0, 2 \rangle.
    \end{split}
  \]
  Taking the cross product, we have:
  \[
    \mathbf{n_2} = \overrightarrow{P_1P_2} \times \overrightarrow{P_1P_3} = \begin{vmatrix}
      \mathbf{i} & \mathbf{j} & \mathbf{k} \\
      -3         & -2         & 2          \\
      -2         & 0          & 2
    \end{vmatrix} = \langle -4, 2, -4 \rangle.
  \]
  For simplicity, we can take the normal vector as $\mathbf{n_2} = \langle 2, -1, 2 \rangle$.

  Then the direction vector of the line $L_2$ is perpendicular to both normal vectors of the planes $\pi_1$ and $\pi_2$. So the direction vector of the line $L_2$ is given by:
  \[
    \mathbf{v_2} = \mathbf{n_1} \times \mathbf{n_2} = \begin{vmatrix}
      \mathbf{i} & \mathbf{j} & \mathbf{k} \\
      1          & -1         & 2          \\
      2          & -1         & 2
    \end{vmatrix} = \langle 0, 2, 1 \rangle.
  \]
  Note that the point $(3, 2, -1)$ lies on two planes, so it also lies on the line $L_2$. Therefore, we can take the point $P_2(3, 2, -1)$ on the line $L_2$. We can calculate the cross product of the direction vectors:
  \[
    \mathbf{v_1} \times \mathbf{v_2} = \begin{vmatrix}
      \mathbf{i} & \mathbf{j} & \mathbf{k} \\
      1          & 2          & 2          \\
      0          & 2          & 1
    \end{vmatrix} = \langle -2, -1, 2 \rangle.
  \]
  Then we can calculate the distance between the two lines $L_1$ and $L_2$ using the formula:
  \[
    \begin{split}
      \text{Distance} & = \frac{|(\mathbf{r_2} - \mathbf{r_1}) \cdot (\mathbf{v_1} \times \mathbf{v_2})|}{\|\mathbf{v_1} \times \mathbf{v_2}\|} = \frac{| \langle 3 - 1, 4 - 2, 0 - 6 \rangle \cdot \langle -2, -1, 2 \rangle |}{\sqrt{{(-2)}^2 + {(-1)}^2 + 2^2}} \\
                      & = \frac{| \langle 2, 2, -6 \rangle \cdot \langle -2, -1, 2 \rangle |}{\sqrt{9}} = \frac{| -4 - 2 - 12 |}{3} = \frac{18}{3} = 6.
    \end{split}
  \]
\end{solution}

\section{Cylinders and Quadric Surfaces}

\subsection{Cylinders}

A cylinder is a surface that consists of all lines that are parallel to a given line and pass through a given curve. The given line is called the \emph{generatrix} of the cylinder, and the given curve is called the \emph{directrix} of the cylinder.

\begin{example}
  Sketch the graph of the surface defined by the equation:
  \[
    z = x^2
  \]
\end{example}
\begin{solution}
  This equation represents a parabolic cylinder. For any fixed value of $y$, the cross-section in the $xz$-plane is a parabola defined by $z = x^2$. The surface extends infinitely along the $y$-axis, forming a cylinder-like shape. Consider the Figure~\ref{fig:parabolic_cylinder} below, which illustrates the parabolic cylinder defined by the equation $z = x^2$. If we take cross-sections at different values of $y$, we obtain parabolas that open upwards in the $xz$-plane.
\end{solution}

\begin{example}
  Sketch the graph of the surface defined by the equation:
  \[
    x^2 + y^2 = 1
  \]
\end{example}
\begin{solution}
  This equation represents a circular cylinder. For any fixed value of $z$, the cross-section in the $xy$-plane is a circle defined by $x^2 + y^2 = 1$. The surface extends infinitely along the $z$-axis, forming a cylinder-like shape. Consider the Figure~\ref{fig:circular_cylinder} below, which illustrates the circular cylinder defined by the equation $x^2 + y^2 = 1$. If we take cross-sections at different values of $z$, we obtain circles in the $xy$-plane.
\end{solution}

\begin{figure}[!ht]
  \centering
  \caption{Cylinders in 3D Space}
  \begin{subfigure}{0.45\textwidth}
    \centering
    \includegraphics{parabolic_cylinder.pdf}
    \caption{Parabolic Cylinder of $z = x^2$}\label{fig:parabolic_cylinder}
  \end{subfigure}
  \begin{subfigure}{0.45\textwidth}
    \centering
    \includegraphics{circular_cylinder.pdf}
    \caption{Circular Cylinder of $x^2 + y^2 = 1$}\label{fig:circular_cylinder}
  \end{subfigure}
\end{figure}

\subsection{Quadric Surfaces}
A quadric surface is a surface in three-dimensional space defined by a second-degree polynomial equation in three variables $x$, $y$, and $z$. The general form of a quadric surface equation is:
\[
  Ax^2 + By^2 + Cz^2 + Dxy + Eyz + Fxz + Gx + Hy + Iz + J = 0.
\]
By simple translation or rotations, it can be brought into one of the following forms:
\[
  Ax^2 + By^2 + Cz^2 + J = 0, \quad Ax^2 + By^2 + Iz = 0
\]

There are 6 kinds of quadric surfaces, as shown below:
\begin{figure}[!ht]

  \centering
  \caption{Quadric Surfaces}

  \begin{subfigure}{0.3\textwidth}
    \centering
    \includegraphics{ellipsoid.pdf}
    \caption{Ellipsoid}
  \end{subfigure}
  \begin{subfigure}{0.3\textwidth}
    \centering
    \includegraphics{elliptic_paraboloid.pdf}
    \caption{Elliptic Paraboloid}
  \end{subfigure}
  \begin{subfigure}{0.3\textwidth}
    \centering
    \includegraphics{hyperbolic_paraboloid.pdf}
    \caption{Hyperbolic Paraboloid}
  \end{subfigure}
  \begin{subfigure}{0.3\textwidth}
    \centering
    \includegraphics{cone.pdf}
    \caption{Cone}
  \end{subfigure}
  \begin{subfigure}{0.3\textwidth}
    \centering
    \includegraphics{hyperboloid_of_one_sheet.pdf}
    \caption{Hyperboloid of One Sheet}
  \end{subfigure}
  \begin{subfigure}{0.3\textwidth}
    \centering
    \includegraphics{hyperboloid_of_two_sheets.pdf}
    \caption{Hyperboloid of Two Sheets}
  \end{subfigure}

\end{figure}

\section{Vector Functions}

A vector function is a function that takes one or more variables and returns a vector. In three-dimensional space, a vector function can be represented as:
\[
  \mathbf{r}(t) = \langle x(t), y(t), z(t) \rangle
\]

The limit of the vector function $\mathbf{r}(t)$ as $t$ approaches $t_0$ is defined as:
\[
  \lim_{t \to t_0} \mathbf{r}(t) = \left\langle \lim_{t \to t_0} x(t), \lim_{t \to t_0} y(t), \lim_{t \to t_0} z(t) \right\rangle
\]

The derivatives of the vector function $\mathbf{r}(t)$ is defined as:
\[
  \dfrac{d\mathbf{r}}{dt} = \mathbf{r}'(t) = \lim_{h \to 0} \frac{\mathbf{r}(t + h) - \mathbf{r}(t)}{h} = \langle x'(t), y'(t), z'(t) \rangle
\]
There are some properties for derivatives of vector functions:
\begin{itemize}
  \item $\dfrac{d}{dt} [\mathbf{u}(t) + \mathbf{v}(t)] = \mathbf{u}'(t) + \mathbf{v}'(t)$
  \item $\dfrac{d}{dt} [c\mathbf{u}(t)] = c\mathbf{u}'(t)$
  \item $\dfrac{d}{dt} [f(t) \mathbf{u}(t)] = f'(t) \mathbf{u}(t) + f(t) \mathbf{u}'(t)$
  \item $\dfrac{d}{dt} [\mathbf{u}(t) \cdot \mathbf{v}(t)] = \mathbf{u}'(t) \cdot \mathbf{v}(t) + \mathbf{u}(t) \cdot \mathbf{v}'(t)$
  \item $\dfrac{d}{dt} [\mathbf{u}(t) \times \mathbf{v}(t)] = \mathbf{u}'(t) \times \mathbf{v}(t) + \mathbf{u}(t) \times \mathbf{v}'(t)$
  \item $\dfrac{d}{dt} [\mathbf{u}(f(t))] = f'(t) \mathbf{u}'(f(t))$
\end{itemize}

The definite integral of vector functions $\mathbf{r}(t)$ from $a$ to $b$ is defined as:
\[
  \int_a^b \mathbf{r}(t) dt = \left\langle \int_a^b x(t) dt, \int_a^b y(t) dt, \int_a^b z(t) dt \right\rangle
\]

We have the following arc length formula for a curve defined by the vector function $\mathbf{r}(t)$ from $t = a$ to $t = b$:
\[
  L = \int_a^b \| \mathbf{r}'(t) \| dt = \int_a^b \sqrt{{(x'(t))}^2 + {(y'(t))}^2 + {(z'(t))}^2} dt
\]

We can parametrise a curve by its arc length. The steps are as follows:

Given a curve $\mathbf{r}(t)$, compute the integral:
\[
  s = s(t) = \int_a^t \| \mathbf{r}'(\tau) \| d\tau
\]
Then express $t$ as a function of $s$, i.e., $t = t(s)$. Lastly replace all $t$ in $\mathbf{r}(t)$ as $\mathbf{r}(t(s))$, a function in terms of $s$.

Note that in the arc-length parametrisation, we have $\| \tilde{\mathbf{r}}'(s) \| = 1$.

\begin{example}
  Find the arc-length parametrisation of the curve:
  \[
    \mathbf{r}(t) = \langle \cos t, \sin t, t \rangle, \qquad t \in [0, 2\pi].
  \]
\end{example}
\begin{solution}
  We have:
  \[
    \| \mathbf{r}'(t) \| = \sqrt{{(-\sin t)}^2 + {(\cos t)}^2 + 1^2} = \sqrt{2}.
  \]
  So,
  \[
    s = \int_0^t \sqrt{2} d\tau = \sqrt{2} t.
  \]
  Express $t$ in terms of $s$, we get $t = \frac{s}{\sqrt{2}}$. Replace all $t$'s in $\mathbf{r}(t)$, we have the arc-length parametrisation:
  \[
    \tilde{\mathbf{r}}(s) = \left\langle \cos\left(\frac{s}{\sqrt{2}}\right), \sin\left(\frac{s}{\sqrt{2}}\right), \frac{s}{\sqrt{2}} \right\rangle, \qquad s \in [0, 2\pi\sqrt{2}].
  \]
\end{solution}


\chapter{Partial Derivatives}

\section{Functions of Several Variables}
For a function of two variables $z = f(x, y)$, the domain is a subset of the $xy$-plane, and the range is a subset of the $z$-axis. The graph of the function is a surface in three-dimensional space defined by the set of points $(x, y, z)$ such that $z = f(x, y)$.

We can consider the ``natural domain'' of the function, which is the largest possible domain on $\mathbb{R}^n$ for which the function is defined for $n$ variable functions. For example, the natural domain of the function $f(x, y) = \sqrt{9 - x^2 - y^2}$ is the disk defined by $x^2 + y^2 \leq 9$. It is to find the largest possible domain on $\mathbb{R}^2$ such that the expression under the square root is non-negative. Then the natural domain is:
\[
  D = \{ (x, y) \mid x^2 + y^2 \leq 9 \}
\]

\section{Level Sets}
Instead of visualising the graph of a function of two variables in three-dimensional space, we can also visualise the function using level curves (or contour curves). A level set of a function $f: \mathbb{R}^n \to \mathbb{R}$ is a subset of the domain where the function takes on a constant value. For a function of two variables $z = f(x, y)$, the level curves are defined by the equation:
\[
  f(x, y) = k
\]

Given $f(x, y) = x^2 + y^2$, an example of level curves is $x^2 + y^2 = 1$, which is the unit circle on $\mathbb{R}^2$ centered at the origin. The level set diagram of the two variables function consists of some representative level sets of function on $\mathbb{R}^2$. The level set diagram of the function $f(x, y) = x^2 + y^2$ is shown in Figure~\ref{fig:level_sets}.

\begin{figure}[!ht]
  \centering
  \includegraphics{level_sets.pdf}
  \caption{Level Sets of $f(x, y) = x^2 + y^2$}\label{fig:level_sets}
\end{figure}

\section{Limit and Continuity}

\begin{definition}[Limits]
  The limit of a function of two variables $f(x, y)$ as $(x, y)$ approaches $(x_0, y_0)$ is $L$ and we write
  \[
    \lim_{(x, y) \to (x_0, y_0)} f(x, y) = L.
  \]
  if for every $\epsilon > 0$, there exists a $\delta > 0$ such that whenever $0 < \| \vec{x} - \vec{x_0} \| < \delta$, it follows that $|f(\vec{x}) - L| < \epsilon$.
\end{definition}

\begin{example}
  Show that the limit below does not exists:
  \[
    \lim_{(x, y) \to (0, 0)} \frac{x^2 - y^2}{x^2 + y^2}.
  \]
\end{example}
\begin{solution}
  Let $f(x, y) = \frac{x^2 - y^2}{x^2 + y^2}$. We will approach the point $(0, 0)$ along two different paths: $x$-axis and $y$-axis.

  \begin{itemize}
    \item Along the $x$-axis ($y = 0$):
          \[
            f(x, 0) = \frac{x^2 - 0^2}{x^2 + 0^2} = \frac{x^2}{x^2} = 1.
          \]
          Thus,
          \[
            \lim_{x \to 0} f(x, 0) = 1.
          \]

    \item Along the $y$-axis ($x = 0$):
          \[
            f(0, y) = \frac{0^2 - y^2}{0^2 + y^2} = \frac{-y^2}{y^2} = -1.
          \]
          Thus,
          \[
            \lim_{y \to 0} f(0, y) = -1.
          \]
  \end{itemize}

  Since the limits along the two different paths are not equal (1 and -1), the limit $\lim_{(x, y) \to (0, 0)} f(x, y)$ does not exist.
\end{solution}

\begin{example}
  Does the limit below exist? If it exists, find the limit.
  \[
    \lim_{(x, y) \to (0, 0)} \frac{xy}{x^2 + y^2}.
  \]
\end{example}
\begin{solution}
  Although approaching along the $x$-axis and $y$-axis both give the limit 0, we need to check other paths to confirm the existence of the limit.

  Let's approach the point $(0, 0)$ along the line $y = mx$, where $m$ is a constant. Substituting $y = mx$ into the function, we have:
  \[
    f(x, mx) = \frac{x(mx)}{x^2 + {(mx)}^2} = \frac{mx^2}{x^2 + m^2x^2} = \frac{mx^2}{x^2(1 + m^2)} = \frac{m}{1 + m^2}.
  \]
  As $x \to 0$, the expression $\frac{m}{1 + m^2}$ remains constant and depends on the value of $m$. Since the limit depends on the slope $m$ of the line we choose to approach $(0, 0)$, the limit does not exist.
\end{solution}

\begin{example}
  Find the limit below, if it exists:
  \[
    \lim_{(x, y) \to (0, 0)} \frac{3x^2 y}{x^2 + y^2}.
  \]
\end{example}
\begin{solution}
  Let $\epsilon > 0$. We need to find a $\delta > 0$ such that whenever $0 < \sqrt{x^2 + y^2} < \delta$, it follows that
  \[
    \left| \frac{3x^2 y}{x^2 + y^2} - 0 \right| < \epsilon \Longleftrightarrow \frac{3x^2 |y|}{x^2 + y^2} < \epsilon.
  \]
  Note that $x^2 \leq x^2 + y^2$, so we have
  \[
    \frac{3x^2 |y|}{x^2 + y^2} \leq 3|y| = 3\sqrt{y^2} \leq 3\sqrt{x^2 + y^2}.
  \]
  Thus, we choose $\delta = \frac{\epsilon}{3}$. Then, whenever $0 < \sqrt{x^2 + y^2} < \delta$, we have
  \[
    \frac{3x^2 |y|}{x^2 + y^2} \leq 3\sqrt{x^2 + y^2} < 3 \cdot \frac{\epsilon}{3} = \epsilon.
  \]
  Therefore, the limit is:
  \[
    \lim_{(x, y) \to (0, 0)} \frac{3x^2 y}{x^2 + y^2} = 0.
  \]
\end{solution}

We have the following properties of limits for functions of several variables:
\begin{itemize}
  \item $\lim_{\vec{x} \to \vec{x}_0} [f(\vec{x}) + g(\vec{x})] = \lim_{\vec{x} \to \vec{x_0}} f(\vec{x}) + \lim_{\vec{x} \to \vec{x_0}} g(\vec{x})$
  \item $\lim_{\vec{x} \to \vec{x_0}} [c f(\vec{x})] = c \lim_{\vec{x} \to \vec{x_0}} f(\vec{x})$
  \item $\lim_{\vec{x} \to \vec{x_0}} [f(\vec{x}) g(\vec{x})] = \left( \lim_{\vec{x} \to \vec{x_0}} f(\vec{x}) \right) \left( \lim_{\vec{x} \to \vec{x_0}} g(\vec{x}) \right)$
  \item $\lim_{\vec{x} \to \vec{x_0}} \left[ \dfrac{f(\vec{x})}{g(\vec{x})} \right] = \dfrac{\lim_{\vec{x} \to \vec{x_0}} f(\vec{x})}{\lim_{\vec{x} \to \vec{x_0}} g(\vec{x})}$, provided that $\lim_{\vec{x} \to \vec{x_0}} g(\vec{x}) \neq 0$.
  \item $\lim_{\vec{x} \to \vec{x_0}} {[f(\vec{x})]}^q = {\left( \lim_{\vec{x} \to \vec{x_0}} f(\vec{x}) \right)}^q$, where $q$ is a rational number.
  \item $\lim_{\vec{x} \to \vec{x_0}} [f(g(\vec{x}))] = f\left( \lim_{\vec{x} \to \vec{x_0}} g(\vec{x}) \right)$, provided that $f$ is continuous at $\lim_{\vec{x} \to \vec{x_0}} g(\vec{x})$.
\end{itemize}
For the last property, functions like polynomials, exponential functions, trigonometric functions, and logarithmic functions are continuous everywhere in their domains.

If we drop the condition that $0 < \| \vec{x} - \vec{x_0} \|$, we get the definition of continuity.

\begin{definition}[Continuity]
  A function $f(x, y)$ is continuous at the point $(x_0, y_0)$ if for every $\epsilon > 0$, there exists a $\delta > 0$ such that whenever $\| \vec{x} - \vec{x_0} \| < \delta$, it follows that $|f(\vec{x}) - f(\vec{x_0})| < \epsilon$.
\end{definition}

\section{Partial Derivatives}

\begin{definition}[Partial Derivatives]
  The partial derivative of a function $f(x, y)$ with respect to $x$ at the point $(x_0, y_0)$ is defined as:
  \[
    f_x(x_0, y_0) = \lim_{h \to 0} \frac{f(x_0 + h, y_0) - f(x_0, y_0)}{h}
  \]
  Similarly, the partial derivative of $f(x, y)$ with respect to $y$ at the point $(x_0, y_0)$ is defined as:
  \[
    f_y(x_0, y_0) = \lim_{h \to 0} \frac{f(x_0, y_0 + h) - f(x_0, y_0)}{h}
  \]
\end{definition}

If we let $(x_0, y_0)$ be any point in the domain of $f(x, y)$, then the partial derivatives $f_x(x_0, y_0)$ and $f_y(x_0, y_0)$ represent the rates of change of the function $f(x, y)$ in the $x$ and $y$ directions, respectively, at that point. We have the following notations for partial derivatives:
\[
  f_x = \dfrac{\partial f}{\partial x} = \partial_x f = D_x f, \quad f_y = \dfrac{\partial f}{\partial y} = \partial_y f = D_y f.
\]

For higher order partial derivatives, we can interchange the order of differentiation if the function is sufficiently smooth (i.e., the mixed partial derivatives are continuous). This is known as Clairaut's theorem or Schwarz's theorem:
\[
  f_{xy} = f_{yx}
\]

\section{Differentiability}

\begin{definition}[Differentiability]
  Given a function $z = f(x, y)$. The function $f$ is \emph{differentiable} at $(x_0, y_0)$ if the partial derivatives $f_x$ and $f_y$ exist in a neighborhood of the point $(x_0, y_0)$ and the following equality holds:
  \[
    f(x, y) - L(x, y) = \epsilon_1(x, y) (x - x_0) + \epsilon_2(x, y) (y - y_0),
  \]
  where $L(x, y)$ is the linear approximation of $f$ at $(x_0, y_0)$, given by this:
  \[
    L(x, y) = f(x_0, y_0) + f_x(x_0, y_0)(x - x_0) + f_y(x_0, y_0)(y - y_0),
  \]
  and $\epsilon_1$ and $\epsilon_2$ are functions such that
  \[
    \lim_{(x, y) \to (x_0, y_0)} \epsilon_1 (x, y) = 0, \quad \lim_{(x, y) \to (x_0, y_0)} \epsilon_2 (x, y) = 0,
  \]
\end{definition}

\section{The Chain Rule and Implicit Differentiation}

Suppose $x = x(t)$ and $y = y(t)$ are differentiable at $t = t_0$, and $z = f(x, y)$ is a differentiable at $(x_0, y_0) = (x(t_0), y(t_0))$. Then the composite function $z = f(x(t), y(t))$ is differentiable with respect to $t$, and its derivative is given by:
\[
  \left.\frac{dz}{dt}\right|_{t = t_0} = \frac{\partial f}{\partial x} (x_0, y_0) \left.\frac{dx}{dt}\right|_{t = t_0} + \frac{\partial f}{\partial y} (x_0, y_0) \left.\frac{dy}{dt}\right|_{t = t_0} = f_x(x_0, y_0) \left.\frac{dx}{dt}\right|_{t = t_0} + f_y(x_0, y_0) \left.\frac{dy}{dt}\right|_{t = t_0}.
\]
\begin{proof}
  Note that from the differentiability of $f$, we have:
  \[
    \begin{split}
      f(x, y) - f(x_0, y_0) & = f_x(x_0, y_0)(x - x_0) + f_y(x_0, y_0)(y - y_0) + \epsilon_1 (x - x_0) + \epsilon_2 (y - y_0) \\
                            & = [ f_x(x_0, y_0) + \epsilon_1 ] (x - x_0) + [ f_y(x_0, y_0) + \epsilon_2 ] (y - y_0).
    \end{split}
  \]
  Then we have:
  \[
    \begin{split}
      \left.\frac{dz}{dt}\right|_{t = t_0} & = \lim_{t \to t_0} \frac{f(x(t), y(t)) - f(x_0, y_0)}{t - t_0}                                                                                                        \\
                                           & = \lim_{t \to t_0} \left[ (f_x(x(t_0), y(t_0)) + \epsilon_1) \frac{x(t) - x(t_0)}{t - t_0} + (f_y(x(t_0), y(t_0)) + \epsilon_2) \frac{y(t) - y(t_0)}{t - t_0} \right] \\
                                           & = f_x(x_0, y_0) \left.\frac{dx}{dt}\right|_{t = t_0} + f_y(x_0, y_0) \left.\frac{dy}{dt}\right|_{t = t_0}.
    \end{split}
  \]
\end{proof}

More generally, if $z = f(x_1, x_2, \ldots, x_n)$ where each $x_i$ is a function of $t$, then:
\[
  \frac{dz}{dt} = \sum_{i=1}^n \frac{\partial f}{\partial x_i} \frac{dx_i}{dt}.
\]

We can draw a tree diagram to visualise the chain rule for functions of several dependent variables with several independent variables. Two examples are shown in Figure~\ref{fig:tree_diagrams}.

\begin{figure}[!ht]
  \centering
  \begin{subfigure}{0.4\textwidth}
    \centering
    \includegraphics{tree_t.pdf}
    \caption{$w = f(x(t), y(t), z(t))$}
  \end{subfigure}
  \begin{subfigure}{0.4\textwidth}
    \centering
    \includegraphics{tree_ts.pdf}
    \caption{$w = f(x(t, s), y(t, s), z(t, s))$}
  \end{subfigure}
  \caption{Tree Diagrams for Chain Rule}\label{fig:tree_diagrams}
\end{figure}

Then we can have the implicit differentiation. Suppose that $w = F(x, y)$ is differentiable and assume $F(x, y) = 0$ defines $y$ as a differentiable function of $x$. Then at any point where $F_y \neq 0$, we have:
\[
  \frac{dy}{dx} = -\frac{F_x}{F_y} = -\frac{\frac{\partial F}{\partial x}}{\frac{\partial F}{\partial y}}.
\]
\begin{proof}
  Let $w = F(x, y) = 0$. As $y$ is implicitly a function of $x$, we can let $y = y(x)$, i.e., $F(x, y) = F(x, y(x)) = 0$. As $w$ is a constant, then by the chain rule, we have:
  \[
    0 = \frac{dw}{dx} = F_x + F_y \frac{dy}{dx}.
  \]
  Rearranging the equation gives the desired result.
\end{proof}

\section{Directional Derivatives and Gradient Vectors}

\begin{definition}[Gradient Vector]
  The gradient vector of a function $f(x, y)$ is defined as:
  \[
    \nabla f(x, y) = \left\langle \frac{\partial f}{\partial x}, \frac{\partial f}{\partial y} \right\rangle = \langle f_x, f_y \rangle
  \]
\end{definition}

If $z = f(x, y)$ is differentiable at $(x_0, y_0)$, then the gradient vector $\nabla f(x_0, y_0)$ is perpendicular to the level curve of $f$ that passes through the point $(x_0, y_0)$.
\begin{proof}
  Let $C$ be the level curve defined by $f(x, y) = k$ that passes through the point $(x_0, y_0)$. Let $\mathbf{r}(t) = \langle x(t), y(t) \rangle$ be a parametrisation of the curve $C$ such that $\mathbf{r}(t_0) = (x_0, y_0)$. Then we differentiation both sides of the equation $f(x(t), y(t)) = k$ with respect to $t$:
  \[
    \frac{df}{dt} = f_x \frac{dx}{dt} + f_y \frac{dy}{dt} = \langle f_x, f_y \rangle \cdot \langle x'(t), y'(t) \rangle = \nabla f(x, y) \cdot \mathbf{r}'(t) = 0.
  \]
  Note that $\mathbf{r}'(t_0)$ is a tangent vector to the curve $C$ at the point $(x_0, y_0)$. Since the dot product of the gradient vector and the tangent vector is zero, it follows that the gradient vector is perpendicular to the level curve at that point.
\end{proof}

We have the following properties of the gradient vector:
\begin{itemize}
  \item $\nabla (f + g) = \nabla f + \nabla g$
  \item $\nabla (c f) = c \nabla f$
  \item $\nabla (f g) = f \nabla g + g \nabla f$
  \item $\nabla \left( \dfrac{f}{g} \right) = \dfrac{g \nabla f - f \nabla g}{g^2}$
\end{itemize}

\begin{definition}[Directional Derivatives]
  The directional derivative of a function $f(x, y)$ at the point $(x_0, y_0)$ in the direction of a unit vector $\mathbf{u} = \langle u_1, u_2 \rangle$ is defined as:
  \[
    D_{\mathbf{u}} f(x_0, y_0) = \lim_{h \to 0} \frac{f(x_0 + hu_1, y_0 + hu_2) - f(x_0, y_0)}{h}
  \]
  Alternatively, it can be computed using the gradient vector:
  \[
    D_{\mathbf{u}} f(x_0, y_0) = \nabla f(x_0, y_0) \cdot \mathbf{u} = f_x(x_0, y_0) u_1 + f_y(x_0, y_0) u_2
  \]
\end{definition}

Note that the maximum and minimum values of the directional derivative occur in the direction of the gradient vector and its opposite direction, respectively, as $D_{\mathbf{u}} f (x_0, y_0) = \nabla f (x_0, y_0) \cdot \mathbf{u}$ and $\|\mathbf{u}\| = 1$. Then we have:
\[
  -\|\nabla f (x_0, y_0)\| \leq D_{\mathbf{u}} f (x_0, y_0) \leq \|\nabla f (x_0, y_0)\|
\]
Then the direction of maximum increase is called the direction of \emph{steepest ascent}, and the direction of maximum decrease is called the direction of \emph{steepest descent}.

\begin{example}
  Suppose that the temperature at the point $(x, y, z)$ in space is given by
  \[
    T(x, y, z) = \frac{80}{1 + x^2 + 2y^2 + 3z^2}.
  \]
  In which direction does the temperature increase fastest at the point $(1, 1, -2)$? What is the maximum rate of increase?
\end{example}
\begin{solution}
  First, we compute the gradient vector:
  \[
    \nabla T(1, 1, -2) = \frac{5}{8} \langle -1, -2, 6 \rangle
  \]
  The direction of steepest ascent is in the direction of the gradient vector, i.e., $\langle -1, -2, 6 \rangle$. The maximum rate of increase is the magnitude of the gradient vector:
  \[
    \|\nabla T(1, 1, -2)\| = \frac{5}{8} \sqrt{{(-1)}^2 + {(-2)}^2 + 6^2} = \frac{5}{8} \sqrt{41}.
  \]
\end{solution}

\begin{example}
  Find the path of the steepest ascent on the surface $f(x, y) = 20 - 4x^2 - y^2$ starting from the point $(2, -3)$.
\end{example}
\begin{solution}
  To find the path of steepest ascent, we need to solve the system of ordinary differential equations given by the gradient vector:
  \[
    \nabla f(x, y) = \langle -8x, -2y \rangle.
  \]
  Thus, we have:
  \[
    \frac{dy}{dx} = \frac{f_y}{f_x} = \frac{-2y}{-8x} = \frac{y}{4x}.
  \]
  Then we can separate the variables and integrate:
  \[
    \frac{4}{y} dy = \frac{1}{x} dx \implies \ln |y|^4 = \ln |x| + C \implies y^4 = K x,
  \]
  where $K = e^C$ is a constant. Using the initial condition $(x, y) = (2, -3)$, we find:
  \[
    81 = K \cdot 2 \implies K = \frac{81}{2}.
  \]
  Therefore, the path of steepest ascent is given by:
  \[
    y^4 = \frac{81}{2} x.
  \]
\end{solution}

\section{Tangent Planes and Linear Approximations}

The equation of the tangent plane to the level surface $k = f(x, y, z)$ at the point $(x_0, y_0, z_0)$ is given by:
\[
  \langle x - x_0, y - y_0, z - z_0 \rangle \cdot \nabla f (x_0, y_0, z_0) = 0,
\]
or equivalently,
\[
  f_x(x_0, y_0, z_0)(x - x_0) + f_y(x_0, y_0, z_0)(y - y_0) + f_z(x_0, y_0, z_0)(z - z_0) = 0.
\]

\begin{example}
  Two surfaces $x^2 + y^2 - 2 = 0$ and $x + z - 4 = 0$ intersect at a curve. Find the equation of the tangent line to the curve of intersection at the point $P_0(1, 1, 3)$.
\end{example}
\begin{solution}
  We first find the normal vectors of the two surfaces at the point $P_0$. For the first surface, we have:
  \[
    \mathbf{n_1} = \nabla f_1 (1, 1, 3) = \langle 2, 2, 0 \rangle.
  \]
  For the second surface, we have $\mathbf{n_2} = \langle 1, 0, 1 \rangle$. Then the direction vector of the tangent line is given by the cross product of the two normal vectors:
  \[
    \mathbf{d} = \mathbf{n_1} \times \mathbf{n_2} = \begin{vmatrix}
      \mathbf{i} & \mathbf{j} & \mathbf{k} \\
      2          & 2          & 0          \\
      1          & 0          & 1
    \end{vmatrix} = \langle 2, -2, -2 \rangle.
  \]
  Therefore, the equation of the tangent line at the point $P_0(1, 1, 3)$ is given by:
  \[
    x = 1 + 2t, \quad y = 1 - 2t, \quad z = 3 - 2t.
  \]
\end{solution}

Recall that the linear approximation of a function $f(x, y)$ at the point $(x_0, y_0)$ is given by:
\[
  L(x, y) = f(x_0, y_0) + f_x(x_0, y_0)(x - x_0) + f_y(x_0, y_0)(y - y_0).
\]

We have the actual change in $f$ given by:
\[
  \Delta f = f(x_0 + \Delta x, y_0 + \Delta y) - f(x_0, y_0),
\]
and the approximate change in $f$ given by:
\[
  df = L(x_0 + \Delta x, y_0 + \Delta y) - L(x_0, y_0) = f_x(x_0, y_0) \Delta x + f_y(x_0, y_0) \Delta y.
\]

When $\Delta x$ and $\Delta y$ are small, $df$ approximates $\Delta f$ well.

We also have the total differential of $f(x, y, z)$ given by:
\[
  df = f_x dx + f_y dy + f_z dz.
\]

\section{Maximum and Minimum Values}

$f(x_0, y_0)$ is a local maximum of $f$ if there exists a neighbourhood $D$ of $(x_0, y_0)$ such that for all $(x, y) \in D$, we have $f(x, y) \leq f(x_0, y_0)$. Similarly, for a local minimum.

If $(x_0, y_0)$ is a local extremum of $f(x, y)$ and the partial derivatives $f_x$ and $f_y$ exist at $(x_0, y_0)$, then:
\[
  f_x(x_0, y_0) = 0, \quad f_y(x_0, y_0) = 0.
\]
Such points are called \emph{critical points}. Note that if either $f_x$ or $f_y$ does not exist at $(x_0, y_0)$, then $(x_0, y_0)$ is also a critical point.
\begin{proof}
  If $f$ has a local extremum at $(x_0, y_0)$, then the function $g(x) = f(x, y_0)$ has a local extremum at $x = x_0$. Hence, by single variable calculus, we have $g'(x_0) = 0$. Then we have $g'(x_0) = f_x(x_0, y_0) = 0$. Similarly, the function $h(y) = f(x_0, y)$ has a local extremum at $y = y_0$, so $h'(y_0) = f_y(x_0, y_0) = 0$.
\end{proof}

A differentiable function $f(x, y)$ has a saddle point at $(x_0, y_0)$ if $(x_0, y_0)$ is a critical point but not a local extremum, i.e., in every neighbourhood of $(x_0, y_0)$, there exist points $(x_1, y_1)$ and $(x_2, y_2)$ such that $f(x_1, y_1) < f(x_0, y_0) < f(x_2, y_2)$.

To classify the critical points, we compute the second partial derivatives of $f$. The second derivative test uses the determinant of the Hessian matrix:
\[
  D = \begin{vmatrix}
    f_{xx} & f_{xy} \\
    f_{yx} & f_{yy}
  \end{vmatrix}
  = f_{xx} f_{yy} - f_{xy}^2.
\]

We have the following cases:

\begin{table}[!ht]
  \centering
  \begin{tabular}{c c | c}
    $\left(f_{xx} f_{yy} - f^2_{xy}\right)\big|_{(x_0, y_0)}$ & $f_{xx}(x_0, y_0)$ & $(x_0, y_0)$ is a \\
    \midrule
    $+$                                                       & $+$                & local minimum     \\
    $+$                                                       & $-$                & local maximum     \\
    $-$                                                       & any                & saddle point      \\
    $0$                                                       & any                & inconclusive      \\
  \end{tabular}
  \caption{Second Derivative Test for Functions of Two Variables}
\end{table}

To find global extrema of a continuous function $f(x, y)$ on a closed and bounded region $R$, we follow these steps:
\begin{enumerate}
  \item Find the critical points of $f$ in the interior of $R$, using the second derivative test to classify them.
  \item Find the maximum and minimum values of $f$ on the boundary of $R$.
  \item Compare all the values obtained in steps 1 and 2 to determine the global maximum and minimum.
\end{enumerate}

\begin{example}
  Find the absolute maximum and minimum values of the function $f(x, y) = 2x^2 - 4x + y^2 - 4y + 1$ on the closed triangular region bounded by the lines $x = 0$, $y = 2$, and $y = 2x$ in the first quadrant.
\end{example}
\begin{solution}
  We first find the interior critical points by setting the first partial derivatives to zero:
  \[
    f_x = 4x - 4 = 0 \implies x = 1, \quad f_y = 2y - 4 = 0 \implies y = 2.
  \]
  Thus, we have one critical point at $(1, 2)$. Next, we compute the second partial derivatives:
  \[
    f_{xx} = 4, \quad f_{yy} = 2, \quad f_{xy} = 0.
  \]
  Then we compute the determinant of the Hessian matrix at $(1, 2)$:
  \[
    D = f_{xx} f_{yy} - f_{xy}^2 = 4 \cdot 2 - 0^2 = 8 > 0, \quad f_{xx} = 4 > 0.
  \]
  Therefore, $(1, 2)$ is a local minimum. Then we evaluate $f$ at this point $f(1, 2) = -5$.

  Then we check the boundary of the triangular region:
  \begin{itemize}
    \item On the line $x = 0$, we have $f(0, y) = y^2 - 4y + 1$. The endpoints are $(0, 0)$ and $(0, 2)$:
          \[
            f(0, 0) = 1, \quad f(0, 2) = -3.
          \]
    \item On the line $y = 2$, we have $f(x, 2) = 2x^2 - 4x + 1$. The endpoints are $(0, 2)$ and $(1, 2)$:
          \[
            f(0, 2) = -3, \quad f(1, 2) = -5.
          \]
    \item On the line $y = 2x$, we have $f(x, 2x) = 4x^2 - 4x + {(2x)}^2 - 4(2x) + 1 = 8x^2 - 12x + 1$. The endpoints are $(0, 0)$ and $(1, 2)$:
          \[
            f(0, 0) = 1, \quad f(1, 2) = -5.
          \]
  \end{itemize}

  Comparing all the values, we find that the absolute maximum value is $1$ at the points $(0, 0)$, and the absolute minimum value is $-5$ at the point $(1, 2)$.
\end{solution}

\section{Lagrange Multipliers}

To find the extrema of a function $f(x, y)$ subject to a constraint $g(x, y) = c$, we introduce a Lagrange multiplier $\lambda$ and solve the system of equations:
\[
  \nabla f(x, y) = \lambda \nabla g(x, y), \quad g(x, y) = c.
\]
\begin{proof}
  Suppose the level curve $g(x, y) = c$ is traced out by a parametrisation $\mathbf{r}(t) = \langle x(t), y(t) \rangle$ with $\mathbf{r}(t_0) = (x_0, y_0)$. Suppose that $f$ has a local extremum at $(x_0, y_0)$ subject to the constraint $g(x, y) = c$. Then we have:
  \[
    0 = \frac{d}{dt} f(x(t), y(t)) \bigg|_{t = t_0} = \nabla f(x_0, y_0) \cdot \mathbf{r}'(t_0).
  \]
  Note that $\mathbf{r}'(t_0)$ is tangent to the level curve $g(x, y) = c$ at $(x_0, y_0)$. Since $\nabla g(x_0, y_0)$ is perpendicular to the level curve at that point, it follows that $\mathbf{r}'(t_0)$ is also perpendicular to $\nabla g(x_0, y_0)$. Therefore, both $\nabla f(x_0, y_0)$ and $\nabla g(x_0, y_0)$ are perpendicular to the same vector $\mathbf{r}'(t_0)$, which implies that they are parallel. Hence, there exists a scalar $\lambda$ such that:
  \[
    \nabla f(x_0, y_0) = \lambda \nabla g(x_0, y_0).
  \]
\end{proof}

If we have more than one constraint, say $g_1(x, y, z) = c_1$ and $g_2(x, y, z) = c_2$, we introduce two Lagrange multipliers $\lambda_1$ and $\lambda_2$ and solve the system of equations:
\[
  \nabla f(x, y, z) = \lambda_1 \nabla g_1(x, y, z) + \lambda_2 \nabla g_2(x, y, z), \quad g_1(x, y, z) = c_1, \quad g_2(x, y, z) = c_2.
\]


\chapter{Multiple Integrals}
\section{Partial Integration}
We have learnt how to calculate the integration of a function in single variable. Now, we extends our knowledge to functions in several variables. One should understand that the partial integration is the reverse process of partial differentiation.

Define a function $f(x,y):\R^2\to\R$, we have
\[\int f \, dx \quad \text{and}\quad \int f \, dy\]
Note that the above integrals are not the same as the single variable integration since $f$ is a function of two variables. The above integrals are called \textbf{partial integrals}. In general, we have

Given a function $f:\R^n \to \R$
\[\int f \, dx_1,\quad \int f \, dx_2,\quad \dots ,\quad \int f \, dx_n\]
where $x_1,x_2,\ldots,x_n$ are the variables of integration.
\begin{example}
  Given a function $f(x,y)=x^2y+3xy^2$, find $\int f \, dx$ and $\int f \, dy$.
  \begin{solution}
    Notice that when we integrate with respect to $x$, we treat $y$ as a constant. So as the other way around. Thus,
    \begin{align*}
      \int x^2y+3xy^2 \, dx & = \frac{y}{3}x^3+\frac{3y^2}{2}x^2 +C(y) \\
      \int x^2y+3xy^2 \, dy & = \frac{x^2}{2}y^2+xy^3 +C(x)
    \end{align*}
  \end{solution}
  The integration constants $C(y)$ and $C(x)$ in this case are functions in $x$ and $y$ rather than just a constant number.
\end{example}

\begin{example}
  Given $f(x,y)=ye^{xy^2}$, find $\int f \, dx$ and $\int f \, dy$.
  \begin{solution}
    \begin{align*}
      \int ye^{xy^2} \, dx & = \frac{e^{xy^2}}{y}+C(y)   \\
      \int ye^{xy^2} \, dy & = \frac{1}{2x}e^{xy^2}+C(x)
    \end{align*}
    We can substitute $u=xy^2$, then $du=y^2dx$ and $du=2xy dy$ to compute the integrals.
  \end{solution}
\end{example}

\section{Definite integration}
The concept here is similar to the single variable definite integration. We define the definite partial integral of $f(x,y)$ with respect to $x$ from $a$ to $b$ as
\[\int_a^b f(x,y) \, dx = \int_{x=a}^{x=b} f(x,y) \, dx= F(b,y)-F(a,y)\]
Similarly, we may define the definite partial integral of $f(x,y)$ with respect to $y$ from $c$ to $d$ as
\[\int_c^d f(x,y) \, dy =\int_{y=c}^{y=d} f(x,y) \, dy= G(x,d)-G(x,c)\]
Note that $y$ and $x$ are treated as constants in the above two definitions respectively.

\begin{example}
  Given $f(x,y)=x^2y+3xy^2$, find $\int_1^3 f(x,y) \, dx$ and $\int_1^3 f(x,y) \, dy$.
  \begin{solution}
    \[
      \begin{split}
        \int_1^3 (x^2y+3xy^2) \, dx & = {\left[\frac{y}{3}x^3+\frac{3y^2}{2}x^2\right]}_{x=1}^{x=3}= \frac{26}{3}y + 12y^2 \\
        \int_1^3 (x^2y+3xy^2) \, dy & = {\left[\frac{x^2}{2}y^2+xy^3\right]}_{y=1}^{y=3}= 4x^2 + 26x
      \end{split}
    \]
  \end{solution}
\end{example}

\section{Double Integrals}
A double integral is an extension of the single variable definite integral to functions of two variables. It is used to calculate the volume under a surface defined by a function $f(x,y)$ over a rectangular region in the $xy$-plane. The double integral of $f(x,y)$ over the rectangular region $R = [a,b] \times [c,d]$ is defined as
\[
  \iint_R f(x,y) \, dA = \lim_{m,n \to \infty} \sum_{i=1}^m \sum_{j=1}^n f(x_{ij}^*, y_{ij}^*) \, \Delta A,
\]
where $\Delta A$ is the area of each subrectangle, and $(x_{ij}^*, y_{ij}^*)$ is a sample point in the $ij$-th subrectangle.

\begin{theorem}[Fubini's Theorem]
  If $f(x,y)$ is continuous on the rectangular region $R = [a,b] \times [c,d]$, then the double integral of $f$ over $R$ can be computed as an iterated integral:
  \[
    \iint_R f(x,y) \, dA = \int_a^b \int_c^d f(x,y) \, dy \, dx = \int_c^d \int_a^b f(x,y) \, dx \, dy.
  \]
  Moreover, this is true if we assume that $f$ is bounded on $R$ and the set of discontinuities of $f$ has measure zero, i.e., $f$ is continuous almost everywhere on $R$.
\end{theorem}

\begin{example}
  Evaluate the double integral $\iint_R (x - 3y^2) \, dA$, where $R = [0, 2] \times [1, 2]$.
\end{example}
\begin{solution}
  There are two ways to evaluate the double integral using iterated integrals.
  \begin{enumerate}
    \item Integrate with respect to $y$ first:
          \begin{align*}
            \iint_R (x - 3y^2) \, dA & = \int_0^2 \int_1^2 (x - 3y^2) \, dy \, dx                                    \\
                                     & = \int_0^2 {\left[ xy - y^3 \right]}_{y=1}^{y=2} \, dx                        \\
                                     & = \int_0^2 (2x - 8 - x + 1) \, dx                                             \\
                                     & = \int_0^2 (x - 7) \, dx = {\left[ \frac{x^2}{2} - 7x \right]}_{0}^{2} = -12.
          \end{align*}
    \item Integrate with respect to $x$ first:
          \begin{align*}
            \iint_R (x - 3y^2) \, dA & = \int_1^2 \int_0^2 (x - 3y^2) \, dx \, dy                           \\
                                     & = \int_1^2 {\left[ \frac{x^2}{2} - 3y^2 x \right]}_{x=0}^{x=2} \, dy \\
                                     & = \int_1^2 (2 - 6y^2) \, dy                                          \\
                                     & = {\left[ 2y - 2y^3 \right]}_{1}^{2} = -12.
          \end{align*}
  \end{enumerate}
\end{solution}

Then consider the double integral over a general region $D$ in the $xy$-plane. If $D$ is a Type I region, i.e., it can be described as
\[
  D = \{ (x,y) \mid a \leq x \leq b, g_1(x) \leq y \leq g_2(x) \},
\]
then the double integral of $f(x,y)$ over $D$ is given by:
\[
  \iint_D f(x,y) \, dA = \int_a^b \int_{g_1(x)}^{g_2(x)} f(x,y) \, dy \, dx.
\]

If $D$ is a Type II region, i.e., it can be described as
\[
  D = \{ (x,y) \mid h_1(y) \leq x \leq h_2(y), c \leq y \leq d \},
\]
then the double integral of $f(x,y)$ over $D$ is given by:
\[
  \iint_D f(x,y) \, dA = \int_c^d \int_{h_1(y)}^{h_2(y)} f(x,y) \, dx \, dy.
\]

Some regions $D$ can be expressed as both Type I and Type II\@. Examples of Type I and Type II regions are shown in Figure~\ref{fig:region_types}.
\begin{figure}[!ht]
  \centering
  \begin{subfigure}{0.45\textwidth}
    \centering
    \includegraphics[width=\linewidth]{type_I_region.pdf}
    \caption{Type I Region}
  \end{subfigure}
  \begin{subfigure}{0.45\textwidth}
    \centering
    \includegraphics[width=\linewidth]{type_II_region.pdf}
    \caption{Type II Region}
  \end{subfigure}
  \caption{Types of Regions for Double Integrals}\label{fig:region_types}
\end{figure}

\begin{example}
  Evaluate the double integral $\iint_D (x + 2y) \, dA$, where $D$ is the region bounded by the parabolas $y = 2x^2$ and $y = 1 + x^2$.
\end{example}
\begin{solution}
  The parabolas intersect when $2x^2 = 1 + x^2$, i.e., $x^2 = 1$, so $x = \pm 1$. Thus, the region $D$ can be described as a Type I region:
  \[
    D = \{ (x,y) \mid -1 \leq x \leq 1, 2x^2 \leq y \leq 1 + x^2 \}.
  \]
  Since $f(x,y) = x + 2y$ is continuous on $D$, by Fubini's Theorem, we have:
  \begin{align*}
    \iint_D (x + 2y) \, dA & = \int_{-1}^1 \int_{2x^2}^{1 + x^2} (x + 2y) \, dy \, dx                                                           \\
                           & = \int_{-1}^1 {\left[ xy + y^2 \right]}_{y=2x^2}^{y=1 + x^2} \, dx                                                 \\
                           & = \int_{-1}^1 \left( x(1 + x^2) + {(1 + x^2)}^2 - x(2x^2) - {(2x^2)}^2 \right) \, dx                               \\
                           & = \int_{-1}^1 \left( -3x^4 - x^3 + 2x^2 + x + 1 \right) \, dx                                                      \\
                           & = {\left[ -\frac{3x^5}{5} - \frac{x^4}{4} +  \frac{2x^3}{3} + \frac{x^2}{2} + x \right]}_{-1}^{1} = \frac{32}{15}.
  \end{align*}
\end{solution}

\begin{example}
  Find the volume of the solid that lies under the paraboloid $z = x^2 + y^2$ and above the region $D$ in the $xy$-plane bounded by the line $y = 2x$ and the parabola $y = x^2$.
\end{example}
\begin{solution}
  There are two ways to describe the region $D$:
  \begin{enumerate}
    \item As a Type I region:
          \[
            D = \{ (x,y) \mid 0 \leq x \leq 2, x^2 \leq y \leq 2x \}.
          \]
          Then the volume is given by:
          \begin{align*}
            V & = \iint_D (x^2 + y^2) \, dA = \int_0^2 \int_{x^2}^{2x} (x^2 + y^2) \, dy \, dx               \\
              & = \int_0^2 {\left[ x^2 y + \frac{y^3}{3} \right]}_{y=x^2}^{y=2x} \, dx                       \\
              & = \int_0^2 \left( 2x^3 + \frac{8x^3}{3} - x^4 - \frac{x^6}{3} \right) \, dx                  \\
              & = {\left[ \frac{7x^4}{6} - \frac{x^5}{5} - \frac{x^7}{21} \right]}_{0}^{2} = \frac{216}{35}.
          \end{align*}
    \item As a Type II region:
          \[
            D = \{ (x,y) \mid \frac{1}{2} y \leq x \leq \sqrt{y}, 0 \leq y \leq 4 \}.
          \]
          Then the volume is given by:
          \begin{align*}
            V & = \iint_D (x^2 + y^2) \, dA = \int_0^4 \int_{\frac{1}{2} y}^{\sqrt{y}} (x^2 + y^2) \, dx \, dy           \\
              & = \int_0^4 {\left[ \frac{x^3}{3} + y^2 x \right]}_{x=\frac{1}{2} y}^{x=\sqrt{y}} \, dy                   \\
              & = \int_0^4 \left( \frac{y^{3/2}}{3} + y^{5/2} - \frac{y^3}{24} - \frac{y^3}{2} \right) \, dy             \\
              & = {\left[ \frac{2y^{5/2}}{15} + \frac{2y^{7/2}}{7} - \frac{13y^4}{96} \right]}_{0}^{4} = \frac{216}{35}.
          \end{align*}
  \end{enumerate}
\end{solution}

If $D = D_1 \cup D_2$, where $D_1$ and $D_2$ are disjoint regions, then we have:
\[
  \iint_D f(x,y) \, dA = \iint_{D_1} f(x,y) \, dA + \iint_{D_2} f(x,y) \, dA.
\]

Similar to single variable calculus, we can change the variables of integration in double integrals. In general, we use the concept of Jacobian determinant to perform the change of variables. Given a transformation $T$ defined by:
\[
  T: (u,v) \mapsto (x,y) = (g(u,v), h(u,v)),
\]
where $g$ and $h$ have continuous first partial derivatives, the Jacobian determinant of the transformation $T$ is defined as:
\[
  J = \frac{\partial (x,y)}{\partial (u,v)} = \begin{vmatrix}
    \dfrac{\partial x}{\partial u} & \dfrac{\partial x}{\partial v} \\
    \dfrac{\partial y}{\partial u} & \dfrac{\partial y}{\partial v}
  \end{vmatrix} = \frac{\partial x}{\partial u} \cdot \frac{\partial y}{\partial v} - \frac{\partial x}{\partial v} \cdot \frac{\partial y}{\partial u}.
\]
Then the double integral of $f(x,y)$ over the region $D$ in the $xy$-plane can be transformed to an integral over the region $S$ in the $uv$-plane as follows:
\[
  \iint_D f(x,y) \, dA = \iint_S f(g(u,v), h(u,v)) \left| \frac{\partial (x,y)}{\partial (u,v)} \right| \, du \, dv.
\]

For example, in polar coordinates, we have the transformation:
\[
  T: (r, \theta) \mapsto (x,y) = (r \cos \theta, r \sin \theta).
\]
The Jacobian determinant is given by:
\[
  J = \begin{vmatrix}
    \cos \theta & -r \sin \theta \\
    \sin \theta & r \cos \theta
  \end{vmatrix} = r.
\]
Thus, the double integral in polar coordinates is given by:
\[
  \iint_D f(x,y) \, dA = \iint_S f(r \cos \theta, r \sin \theta) \, r \, dr \, d\theta.
\]

\begin{example}
  Evaluate the double integral $\iint_R (3x + 4y^2) \, dA$, where $R$ is the region in the upper half-plane bounded by the circles $x^2 + y^2 = 1$ and $x^2 + y^2 = 4$.
\end{example}
\begin{solution}
  The region $R$ can be described in polar coordinates as:
  \[
    R = \{ (r, \theta) \mid 1 \leq r \leq 2, 0 \leq \theta \leq \pi \}.
  \]
  Then the double integral becomes:
  \begin{align*}
    \iint_R (3x + 4y^2) \, dA & = \int_0^{\pi} \int_1^2 \left( 3r \cos \theta + 4r^2 \sin^2 \theta \right) \, r \, dr \, d\theta                            \\
                              & = \int_0^{\pi} {\left[ r^3 \cos \theta + r^4 \sin^2 \theta \right]}_{r=1}^{r=2} \, d\theta                                  \\
                              & = \int_0^{\pi} \left( 7 \cos \theta + 15 \sin^2 \theta \right) \, d\theta                                                   \\
                              & = {\left[ 7 \sin \theta + 15 \left( \frac{\theta}{2} - \frac{\sin 2\theta}{4} \right) \right]}_{0}^{\pi} = \frac{15\pi}{2}.
  \end{align*}
\end{solution}

Given a parametric surface defined by the vector function:
\[
  \mathbf{r}(u,v) = \langle g(u,v), h(u,v), k(u,v) \rangle,
\]
the surface area of the surface over the region $R$ in the $uv$-plane is given by:
\[
  S = \lim_{m,n \to \infty} \sum_{i=1}^m \sum_{j=1}^n \, \Delta T_{ij},
\]
where $\Delta T_{ij}$ is the area of the parallelogram formed by the tangent vectors $\mathbf{r}_u$ and $\mathbf{r}_v$ at the point $(u_{ij}^*, v_{ij}^*)$ in the $ij$-th subrectangle. Then we have:
\[
  \Delta T_{ij} \approx \left| \mathbf{r}_u \times \mathbf{r}_v \right| \, \Delta u \, \Delta v.
\]
Thus, the surface area is given by the double integral:
\[
  S = \iint_R \left| \mathbf{r}_u \times \mathbf{r}_v \right| \, du \, dv.
\]

If the surface is given by the graph of a function $z = f(x,y)$ over the region $D$ in the $xy$-plane, then the cross product of the tangent vectors is given by:
\[
  \mathbf{r}_x \times \mathbf{r}_y = \begin{vmatrix}
    \mathbf{i} & \mathbf{j} & \mathbf{k} \\
    1          & 0          & f_x        \\
    0          & 1          & f_y
  \end{vmatrix} = \langle -f_x, -f_y, 1 \rangle,
\]
and its magnitude is given by:
\[
  \left| \mathbf{r}_x \times \mathbf{r}_y \right| = \sqrt{1 + f_x^2 + f_y^2}.
\]
Therefore, the surface area of the surface $z = f(x,y)$ over the region $D$ is given by:
\[
  S = \iint_D \sqrt{1 + f_x^2 + f_y^2} \, dA.
\]

\begin{example}
  Find the area of the ellipse cutting from the plane $z = 100 - x - y$ by the vertical cylinder $x^2 + y^2 = 1$.
\end{example}
\begin{solution}
  The surface can be described by the function $f(x,y) = 100 - x - y$. Then we have:
  \[
    f_x = -1, \quad f_y = -1.
  \]
  Also, the region $R$ in the $xy$-plane is the disk $x^2 + y^2 \leq 1$. Thus, the surface area is given by:
  \begin{align*}
    S & = \iint_R \sqrt{1 + f_x^2 + f_y^2} \, dA = \iint_R \sqrt{1 + {(-1)}^2 + {(-1)}^2} \, dA = \sqrt{3} \iint_R \, dA \\
      & = \sqrt{3} \cdot \text{Area of } R = \sqrt{3} \cdot \pi {(1)}^2 = \pi \sqrt{3}.
  \end{align*}
\end{solution}

\begin{example}
  Find the area of the portion of the surface $z = 1 - x^2 + y$ that lies above the triangular region $R$ with vertices at $(0,0,0)$, $(0,-1,0)$ and $(1,0,0)$.
\end{example}
\begin{solution}
  The surface can be described by the function $f(x,y) = 1 - x^2 + y$. Then we have:
  \[
    f_x = -2x, \quad f_y = 1.
  \]
  Also, the region $R$ in the $xy$-plane is the triangle bounded by the lines $y = 0$, $x = 0$, and $y = x-1$. Thus, the surface area is given by:
  \begin{align*}
    S & = \iint_R \sqrt{1 + f_x^2 + f_y^2} \, dA = \iint_R \sqrt{1 + {(-2x)}^2 + {1}^2} \, dA = \iint_R \sqrt{2 + 4x^2} \, dA                           \\
      & = \int_0^1 \int_{x-1}^0 \sqrt{2 + 4x^2} \, dy \, dx = \int_0^1 {\left[ y \sqrt{2 + 4x^2} \right]}_{y=x-1}^{y=0} \, dx                           \\
      & = \int_0^1 (1 - x) \sqrt{2 + 4x^2} \, dx = \int_0^1 \sqrt{2 + 4x^2} \, dx - \int_0^1 x \sqrt{2 + 4x^2} \, dx                                    \\
      & = {\left[ \frac{x\sqrt{2 + 4x^2}}{2} + \frac{1}{2} \ln (2x + \sqrt{2 + 4x^2}) \right]}_0^1 - {\left[ \frac{{(2 + 4x^2)}^{3/2}}{12} \right]}_0^1 \\
      & = \frac{\sqrt{6}}{3} + \frac{1}{2} \ln (2 + \sqrt{6}) - \frac{2\sqrt{2}}{3}.
  \end{align*}
\end{solution}

\section{Triple Integrals}
A triple integral is an extension of the double integral to functions of three variables. It is used to calculate the volume under a surface defined by a function $f(x,y,z)$ over a rectangular box in three-dimensional space. The triple integral of $f(x,y,z)$ over the solid region $R = [a,b] \times [c,d] \times [e,f]$ is defined as:
\[
  \iiint_R f(x, y, z) \, dV = \lim_{l,m,n \to \infty} \sum_{i=1}^l \sum_{j=1}^m \sum_{k=1}^n f(x_{ijk}^*, y_{ijk}^*, z_{ijk}^*) \, \Delta V,
\]
where $\Delta V$ is the volume of each sub-box, and $(x_{ijk}^*, y_{ijk}^*, z_{ijk}^*)$ is a sample point in the $ijk$-th sub-box.

By the extension of Fubini's Theorem, if $f(x,y,z)$ is continuous on the rectangular box $R$, then the triple integral of $f$ over $R$ can be computed as an iterated integral:
\[
  \iiint_R f(x,y,z) \, dV = \int_a^b \int_c^d \int_e^f f(x,y,z) \, dz \, dy \, dx.
\]
There are six possible orders of integration for the triple integral, and any of them can be used to evaluate the integral.

Moreover, this is true if we assume that $f$ is bounded on $R$ and the set of discontinuities of $f$ has measure zero, i.e., $f$ is continuous almost everywhere on $R$.

Similarly, there are three types of regions in three-dimensional space, which can be described using inequalities involving the variables $x$, $y$, and $z$. One may consider the projection of the solid region onto the $xy$, $yz$, and $xz$ planes to determine the limits of integration for each variable.

\begin{example}
  Evaluate $\iiint_R \sqrt{x^2 + z^2} dV$, where $R$ is the region bounded by the paraboloid $y = x^2 + z^2$ and the plane $y = 4$.
\end{example}
\begin{solution}
  Consider the projection of the solid region $R$ onto the $xz$-plane. The projection is the disk $x^2 + z^2 \leq 4$. Thus, we can describe the region $R$ as:
  \[
    R = \{ (x,y,z) \mid x^2 + z^2 \leq 4, x^2 + z^2 \leq y \leq 4 \}.
  \]
  Then the triple integral becomes:
  \begin{align*}
    \iiint_R \sqrt{x^2 + z^2} \, dV & = \iint_{x^2 + z^2 \leq 4} \int_{y = x^2 + z^2}^{y = 4} \sqrt{x^2 + z^2} \, dy \, dA         \\
                                    & = \iint_{x^2 + z^2 \leq 4} {\left[ y \sqrt{x^2 + z^2} \right]}_{y = x^2 + z^2}^{y = 4} \, dA \\
                                    & = \iint_{x^2 + z^2 \leq 4} (4 - x^2 - z^2) \sqrt{x^2 + z^2} \, dA.
  \end{align*}
  Now we convert to polar coordinates:
  \begin{align*}
    \iiint_R \sqrt{x^2 + z^2} \, dV & = \int_0^{2\pi} \int_0^2 (4 - r^2) \, r \cdot r \, dr \, d\theta \\
                                    & = \int_0^{2\pi} \, d\theta \, \int_0^2 (4r^2 - r^4) \, dr        \\
                                    & = 2\pi {\left[ \frac{4r^3}{3} - \frac{r^5}{5} \right]}_{0}^{2}   \\
                                    & = \frac{128\pi}{15}.
  \end{align*}
\end{solution}

Similar to double integrals, we can change the variables of integration in triple integrals using the Jacobian determinant. Given a transformation $T$ defined by:
\[
  T: (u,v,w) \mapsto (x,y,z) = (g(u,v,w), h(u,v,w), k(u,v,w)),
\]
where $g$, $h$, and $k$ have continuous first partial derivatives, the Jacobian determinant of the transformation $T$ is defined as:
\[
  J = \frac{\partial (x,y,z)}{\partial (u,v,w)} = \begin{vmatrix}
    \dfrac{\partial x}{\partial u} & \dfrac{\partial x}{\partial v} & \dfrac{\partial x}{\partial w} \\
    \dfrac{\partial y}{\partial u} & \dfrac{\partial y}{\partial v} & \dfrac{\partial y}{\partial w} \\
    \dfrac{\partial z}{\partial u} & \dfrac{\partial z}{\partial v} & \dfrac{\partial z}{\partial w}
  \end{vmatrix}.
\]
Then the triple integral of $f(x,y,z)$ over the region $R$ in the $xyz$-space can be transformed to an integral over the region $S$ in the $uvw$-space as follows:
\[
  \iiint_R f(x,y,z) \, dV = \iiint_S f(g(u,v,w), h(u,v,w), k(u,v,w)) \left| \frac{\partial (x,y,z)}{\partial (u,v,w)} \right| \, du \, dv \, dw.
\]

For cylindrical coordinates, we have the transformation:
\[
  T: (r, \theta, z) \mapsto (x,y,z) = (r \cos \theta, r \sin \theta, z).
\]
where $r$ is the distance from the $z$-axis to the point, $\theta$ is the angle in the $xy$-plane from the positive $x$-axis, and $z$ is the height above the $xy$-plane. The Jacobian determinant is given by:
\[
  J = \begin{vmatrix}
    \cos \theta & -r \sin \theta & 0 \\
    \sin \theta & r \cos \theta  & 0 \\
    0           & 0              & 1
  \end{vmatrix} = r.
\]
Hence the triple integral in cylindrical coordinates is given by:
\[
  \iiint_R f(x,y,z) \, dV = \iiint_S f(r \cos \theta, r \sin \theta, z) \, r \, dr \, d\theta \, dz.
\]

For spherical coordinates, we have the transformation:
\[
  T: (\rho, \theta, \phi) \mapsto (x,y,z) = (\rho \sin \phi \cos \theta, \rho \sin \phi \sin \theta, \rho \cos \phi).
\]
where $\rho$ is the distance from the origin to the point, $\theta$ is the angle in the $xy$-plane from the positive $x$-axis, and $\phi$ is the angle from the positive $z$-axis. The Jacobian determinant is given by:
\[
  J = \begin{vmatrix}
    \sin \phi \cos \theta & \rho \sin \phi (-\sin \theta) & \rho \cos \phi \cos \theta \\
    \sin \phi \sin \theta & \rho \sin \phi \cos \theta    & \rho \cos \phi \sin \theta \\
    \cos \phi             & 0                             & -\rho \sin \phi
  \end{vmatrix} = \rho^2 \sin \phi.
\]
Thus, the triple integral in spherical coordinates is given by:
\[
  \iiint_R f(x,y,z) \, dV = \iiint_S f(\rho \sin \phi \cos \theta, \rho \sin \phi \sin \theta, \rho \cos \phi) \, \rho^2 \sin \phi \, d\rho \, d\theta \, d\phi.
\]


\chapter{Vector Calculus}

It is an introduction to the Calculus on Manifolds. We will cover vector fields, line integrals, surface integrals, Green's theorem, Stokes' theorem, and the Divergence theorem.

\section{Vector Fields}
A vector field is a vector function that assigns a vector to each point in a subset of space. In two dimensions, a vector field can be represented as:
\[
  \mathbf{F}(x,y) = \langle P(x,y), Q(x,y) \rangle,
\]
where $P(x,y)$ and $Q(x,y)$ are scalar functions representing the components of the vector field in the $x$ and $y$ directions, respectively. In three dimensions, a vector field can be represented as:
\[
  \mathbf{F}(x,y,z) = \langle P(x,y,z), Q(x,y,z), R(x,y,z) \rangle,
\]
where $P(x,y,z)$, $Q(x,y,z)$, and $R(x,y,z)$ are scalar functions that represent the components of the vector field in the $x$, $y$, and $z$ directions, respectively.

We say $\mathbf{F}$ is a conservative vector field if there exists a scalar potential function $\phi$ such that $\mathbf{F} = \nabla \phi$, where $\nabla \phi$ is the gradient of $\phi$. Given a vector field $\mathbf{F}(x,y) = \langle P(x,y), Q(x,y) \rangle$, if $P$ and $Q$ have continuous first partial derivatives, we can find the potential function $\phi$ by solving the following system of equations:
\[
  \frac{\partial \phi}{\partial x} = P(x,y), \quad \frac{\partial \phi}{\partial y} = Q(x,y).
\]

\section{Line Integrals}
\subsection{Line Integrals of scalar functions}
A line integral is an extension of the definite integral to functions defined along a curve in space. Given a scalar function $f(x,y)$ and a curve $C$ parameterised by $\mathbf{r}(t) = \langle x(t), y(t) \rangle$ for $a \leq t \leq b$, the line integral of $f$ along $C$ is defined as:
\[
  \int_C f(x,y) \, ds = \int_a^b f(x(t), y(t)) \left| \mathbf{r}'(t) \right| \, dt,
\]
where $s$ is the arc length parameter along the curve $C$.

\begin{example}
  Evaluate $\int_C (2 + x^2 y) \, ds$ where $C$ is the upper half of the unit circle $x^2 + y^2 = 1$.
\end{example}
\begin{solution}
  We can parameterise the upper half of the unit circle as:
  \[
    \mathbf{r}(t) = \langle \cos t, \sin t \rangle, \quad 0 \leq t \leq \pi.
  \]
  Then we have:
  \[
    \mathbf{r}'(t) = \langle -\sin t, \cos t \rangle, \quad \left| \mathbf{r}'(t) \right| = \sqrt{(-\sin t)^2 + (\cos t)^2} = 1.
  \]
  Thus, the line integral becomes:
  \begin{align*}
    \int_C (2 + x^2 y) \, ds & = \int_0^{\pi} (2 + (\cos t)^2 (\sin t)) \cdot 1 \, dt                     \\
                             & = \int_0^{\pi} (2 + \cos^2 t \sin t) \, dt                                 \\
                             & = {\left[ 2t - \frac{\cos^3 t}{3} \right]}_0^{\pi} = 2\pi + \frac{2}{3}.
  \end{align*}
\end{solution}

When $C$ is only piecewise smooth, we can break the line integral into several integrals over each smooth segment and sum them up, i.e., if $C = C_1 \cup C_2 \cup \cdots \cup C_n = \bigcup_{i=1}^n C_i$, then
\[
  \int_C f(x,y) \, ds = \int_{C_1} f(x,y) \, ds + \int_{C_2} f(x,y) \, ds + \cdots + \int_{C_n} f(x,y) \, ds = \sum_{i=1}^n \int_{C_i} f(x,y) \, ds.
\]

Three-dimensional line integrals are defined similarly. Given a scalar function $f(x,y,z)$ and a curve $C$ parameterised by $\mathbf{r}(t) = \langle x(t), y(t), z(t) \rangle$ for $a \leq t \leq b$, the line integral of $f$ along $C$ is defined as:
\[
  \int_C f(x,y,z) \, ds = \int_a^b f(x(t), y(t), z(t)) \left| \mathbf{r}'(t) \right| \, dt,
\]
where $s$ is the arc length parameter along the curve $C$.

\subsection{Line Integrals of vector fields}

Let $\mathbf{F}(x,y) = \langle P(x,y), Q(x,y) \rangle$ be a vector field in two dimensions, and let $C$ be a smooth curve parameterised by $\mathbf{r}(t) = \langle x(t), y(t) \rangle$ for $a \leq t \leq b$. The line integral of the vector field $\mathbf{F}$ along the curve $C$ is defined as:
\[
  \int_C \mathbf{F} \cdot \mathbf{T} \, ds = \int_a^b \mathbf{F}(\mathbf{r}(t)) \cdot \mathbf{r}'(t) \, dt,
\]
where $\mathbf{T}$ is the unit tangent vector to the curve $C$. There are alternative notations for the line integral of a vector field, such as:
\[
  \int_C \mathbf{F} \cdot \mathbf{T} \, ds = \int_C \mathbf{F} \cdot d\mathbf{r} = \int_C P \, dx + Q \, dy.
\]

Similar to two dimensions, let $\mathbf{F}(x,y,z) = \langle P(x,y,z), Q(x,y,z), R(x,y,z) \rangle$ be a vector field in three dimensions, and let $C$ be a smooth curve parameterised by $\mathbf{r}(t) = \langle x(t), y(t), z(t) \rangle$ for $a \leq t \leq b$. The line integral of the vector field $\mathbf{F}$ along the curve $C$ is defined as:
\[
  \int_C \mathbf{F} \cdot \mathbf{T} \, ds = \int_a^b P dx + Q dy + R dz,
\]
where $\mathbf{T}$ is the unit tangent vector to the curve $C$.

Then we introduce the generalised Stokes' Theorem, which relates the Fundamental Theorem of Calculus, Fundamental Theorem of Line Integrals, Green's Theorem, the Divergence Theorem, and Stokes' Theorem into a single theorem. It states that for a smooth manifold $\mathcal{M}$ with boundary $\partial \mathcal{M}$, and a differential form $\omega$ defined on $\mathcal{M}$, we have:
\[
  \int_{\mathcal{M}} d\omega = \int_{\partial \mathcal{M}} \omega.
\]
We may define the exterior derivative $d\omega$ of the differential form $\omega$, which generalises the concepts of gradient, curl, and divergence, with the following properties:
\begin{itemize}
  \item $d$ satisfies linearity: $d(\alpha \omega_1 + \beta \omega_2) = \alpha d\omega_1 + \beta d\omega_2$ for any differential forms $\omega_1$ and $\omega_2$, and scalars $\alpha$ and $\beta$.
  \item $d$ satisfies the product rule: $d(\omega_1 \wedge \omega_2) = d\omega_1 \wedge \omega_2 + (-1)^k \omega_1 \wedge d\omega_2$, where $\omega_1$ is a $k$-form.
  \item $d$ is nilpotent: $d(d\omega) = 0$ for any differential form $\omega$.
  \item $dy \wedge dx = - dx \wedge dy$ for any differential forms $dx$ and $dy$.
  \item $dP(x,y) = P_x \, dx + P_y \, dy$ for any scalar function $P(x,y)$.
\end{itemize}

\begin{theorem}[Fundamental Theorem of Line Integrals]
  Let $f$ be a differentiable scalar function defined on a smooth curve $C$ parameterised by the vector function $\mathbf{r}(t)$ for $a \leq t \leq b$ and the vector function $\nabla f$ is continuous n $C$. Then we have:
  \[
    \int_C \nabla f \cdot d\mathbf{r} = f(\mathbf{r}(b)) - f(\mathbf{r}(a)).
  \]
\end{theorem}
\begin{proof}
  We have:
  \begin{align*}
    \int_C \nabla f \cdot d\mathbf{r} & = \int_a^b \nabla f(\mathbf{r}(t)) \cdot \mathbf{r}'(t) \, dt               \\
                                      & = \int_a^b \frac{d}{dt} f(\mathbf{r}(t)) \, dt                           \\
                                      & = f(\mathbf{r}(b)) - f(\mathbf{r}(a)).
  \end{align*}
\end{proof}

Then we may consider the differential 0-form $\omega = f$ on the curve $C$, and its exterior derivative is given by:
\[
  d\omega = df = \nabla f \cdot d\mathbf{r}.
\]
Thus, by the generalised Stokes' Theorem, we have:
\[
  \int_C \nabla f \cdot d\mathbf{r} = \int_{\partial C} f = f(\mathbf{r}(b)) - f(\mathbf{r}(a)).
\]

Then the Fundamental Theorem of Line Integrals implies that the line integral of a conservative vector field is path-independent, i.e., the value of the line integral depends only on the endpoints of the curve and not on the specific path taken between them. Actually, $\mathbf{F}$ is a conservative vector field if and only if the line integral of $\mathbf{F}$ is path-independent.

\begin{theorem}[Green's Theorem]
  Let $C$ be a positively oriented, piecewise smooth, closed curve in the plane, and let $D$ be the region bounded by $C$. If $\mathbf{F}(x,y) = \langle P(x,y), Q(x,y) \rangle$ is a vector field with continuous partial derivatives on an open region that contains $D$, then we have:
  \[
    \oint_C \mathbf{F} \cdot d\mathbf{r} = \iint_D (\nabla \times \mathbf{F}) \cdot \mathbf{k} \, dA = \iint_D \left( \frac{\partial Q}{\partial x} - \frac{\partial P}{\partial y} \right) \, dA.
  \]
\end{theorem}

Consider the differential 1-form $\omega = P \, dx + Q \, dy$ on the region $D$, and its exterior derivative is given by:
\begin{align*}
  d\omega &= dP \, dx + P \, d(dx) + dQ \, dy + Q \, d(dy) \\
  &= (P_x \, dx + P_y \, dy) \wedge dx + (Q_x \, dx + Q_y \, dy) \wedge dy \\
  &= P_x \, dx \wedge dx + P_y \, dy \wedge dx + Q_x \, dx \wedge dy + Q_y \, dy \wedge dy \\
  &= (Q_x - P_y) \, dx \wedge dy.
\end{align*}
Thus, by the generalised Stokes' Theorem, we have:
\[
  \oint_C P \, dx + Q \, dy = \iint_D (Q_x - P_y) \, dx \, dy.
\]








\end{document}